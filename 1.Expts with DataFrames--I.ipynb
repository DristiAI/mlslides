{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last amended: 26th August, 2020\n",
    "# My folder: /home/ashok/Documents/spark/1.basics\n",
    "# Ref:\n",
    "# Databricks:\n",
    "#     https://docs.databricks.com/spark/latest/dataframes-datasets/introduction-to-dataframes-python.html\n",
    "# Medium\n",
    "#     https://towardsdatascience.com/pyspark-and-sparksql-basics-6cb4bf967e53\n",
    "# Github\n",
    "#     https://github.com/pinarersoy/PySpark_SparkSQL_MLib/blob/master/PySpark%20and%20SparkSQL.ipynb    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.0 Call libraries\n",
    "from pyspark.sql import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More memory to Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check memory allocated by\n",
    "# clicking on Spark_UI hyperlink\n",
    "# See :  https://stackoverflow.com/a/62737941\n",
    "\n",
    "# To allocate more memory to spark, start \n",
    "# this notebook as (and not as pysparknb):\n",
    "# \n",
    "#     pyspark --driver-memory 3g \n",
    "\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Display outputs from multiple commands\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Before using Row class, have a look at its help\n",
    "help(Row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 Row class\n",
    "# Refer: https://spark.apache.org/docs/1.1.1/api/python/pyspark.sql.Row-class.html\n",
    "# Create row of data each. Keyword is not within inverted comma\n",
    "# Each Row object is unrelated to another. At the moment\n",
    "# there is no DataFrame\n",
    "student1 = Row(rollno = '001', name = 'rajeev' , age = 20, income = 80)\n",
    "student2 = Row(rollno = '002', name = 'divakar', age = 40, income = None)  # Missing value\n",
    "student3 = Row(rollno = \"003\", name = 'smitha',  age = 80, income = 70)\n",
    "student4 = Row(rollno = None , name = 'murali',  age = 20, income = 80)     # Missing value\n",
    "student5 = Row(rollno = '005', name = 'mist',    age = 40, income = 90)\n",
    "student6 = Row(rollno = '006', name = \"ragini\",  age = 80, income = None)  # Missing value\n",
    "student7 = Row(rollno = '007', name = \"ravi\",    age = None, income = None)     # Missing values\n",
    "student8 = Row(rollno = '008', name = \"pujari\",  age = 47, income = 30)\n",
    "student9 = Row(rollno = '009', name = \"puneet\",  age = 47, income = 60)\n",
    "student10 =Row(rollno = '010', name = \"gautam\",  age = 47, income = 55)\n",
    "student11 =Row(rollno = '011', name = \"fauji\",   age = 36, income = 21)\n",
    "student12 =Row(rollno = '010', name = \"gautam\",  age = 36, income = 76)\n",
    "student13 =Row(rollno = '012', name = \"major\",   age = 36, income = 32)\n",
    "student14 =Row(rollno = '013', name = None,      age = 66, income = 44)      # Missing value\n",
    "student15 =Row(rollno = '014', name = \"rehman\",  age = 66, income = 43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4 Row class\n",
    "# Create an object to create rows. Keywords are in inverted commas\n",
    "# and there is no '=' to sign. Table of disciplines\n",
    "disc = Row(\"disc_name\", \"faculty\")\n",
    "dept1 = disc('QT', 'Dr. Vibhu')\n",
    "dept2 = disc('IT', 'Dr. Nath')\n",
    "dept3 = disc('HR', 'Dr. Siva')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Row' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c80b6e636006>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msection1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstudent1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstudent2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstudent3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstudent7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstudent8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstudent9\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mdepts\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdept1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdept2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msection2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstudent1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstudent3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstudent4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstudent5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstudent6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstudent10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepts\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdept2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdept3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msection3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstudent2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstudent4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstudent6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstudent8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstudent10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstudent11\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdept1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdept3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Row' is not defined"
     ]
    }
   ],
   "source": [
    "section1 = Row(students = [student1, student2, student3, student7, student8,student9] , depts  = [dept1,dept2] )\n",
    "section2 = Row(students = [student1, student3, student4, student5, student6,student10], depts  = [dept2,dept3])\n",
    "section3 = Row(students = [student2, student4, student6, student8, student10,student11], depts = [dept1,dept3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.5 Extract data from Rows\n",
    "#     Syntax: rowname[key]\n",
    "section1\n",
    "section2.students\n",
    "section3['students']\n",
    "section3['students'][0].name\n",
    "type(student1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframe with list of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.0 Method createDataFrame\n",
    "#help(spark.createDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Create a list of Row objects\n",
    "#     and use the list to create a spark DataFrame\n",
    "students1 = [student1, student2,student3,student4,student5,student6,student7,student8,student9,student10,student11]\n",
    "students1 = spark.createDataFrame(students1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1.1 Another DataFrame\n",
    "students2 = [student13, student14,student15]\n",
    "students2 = spark.createDataFrame(students2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2\n",
    "type(students1)\n",
    "# 2.3\n",
    "students1.show()\n",
    "students2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4 Two other dataframes\n",
    "departments = [dept1,dept2]\n",
    "departments = spark.createDataFrame(departments)\n",
    "sections = [section1,section2,section3]\n",
    "sections = spark.createDataFrame(sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4.1\n",
    "departments.show()\n",
    "sections.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.0 Creating DataFrame from dictionary\n",
    "#     But all dicts must be collected in a list\n",
    "\n",
    "spark.createDataFrame([{ 'a' : 1, 'b' : 3},\n",
    "                       { 'a': 4, 'b' : 7}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 This creates a DataFrame. Schema can be inferred\n",
    "spark.createDataFrame([{ 'a': [1,4], 'b' : [3,7]}])\n",
    "# 3.1.1 This does not create a DataFrame. Schema cannot be inferred\n",
    "# spark.createDataFrame([1,2,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 This also does not create dataframe. \n",
    "#      Schema cannot be inferred\n",
    "# spark.createDataFrame({ 'a': [1,4], 'b' : [3,7]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.0\n",
    "# Stack vertically spark dataframes. Use Union\n",
    "students = students1.union(students2)\n",
    "students.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1\n",
    "# Ref: Spark sql functions:\n",
    "#       https://spark.apache.org/docs/latest/api/sql/index.html\n",
    "from pyspark.sql.functions import explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explodeDF = sections.select(explode(\"students\").alias(\"e\"))\n",
    "explodeDF.show(2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explodeDF = sections.select(explode(\"students\").alias(\"e\"))\n",
    "explodeDF.show()\n",
    "type(explodeDF)\n",
    "explodeDF.select(\"e\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 selectExpr takes \n",
    "#     'SQL expressions as a string':\n",
    "help(explodeDF.selectExpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 \n",
    "explodeDF.show(2)\n",
    "flattenDF= explodeDF.selectExpr(\"e.rollno\",\"e.name\", \"e.age\", \"e.income\")\n",
    "flattenDF1= explodeDF.selectExpr(\"e.rollno\",\"e.name\", \"e.age /60 \", \"e.income/100 \")\n",
    "flattenDF.show()\n",
    "flattenDF1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4 Another explode() example\n",
    "ashok   = Row(mobile = [\"8750\", \"9876\", \"3423\", \"2323\"],     name = \"ashok\"  )\n",
    "pradeep = Row(mobile = [\"18750\", \"19876\", \"13423\", \"12323\"], name = \"pradeep\")\n",
    "first = spark.createDataFrame([ashok,pradeep])\n",
    "first.show()\n",
    "first.select(explode(\"mobile\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.0 Filter conditions must be within inverted commas\n",
    "flattenDF.filter(\"age == 20 and name = 'ragini'\").show()\n",
    "flattenDF.filter(\"age == 20 OR name = 'pujari'\").show()\n",
    "flattenDF.filter(\"age != 20 \").bshow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Another way to filter\n",
    "flattenDF[(flattenDF.age == 20)  &  (flattenDF.name == 'rajeev')].show()\n",
    "flattenDF[(flattenDF.age == 20)  |  (flattenDF.name == 'pujari')].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.0 Using col function\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Another way to write filter condition\n",
    "flattenDF.filter(col(\"age\") == 20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 USe of 'where' is same as 'filter'\n",
    "flattenDF.where(col(\"name\") == \"ragini\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3 Check where age is null\n",
    "flattenDF.filter(col(\"age\").isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7.0 Data aggregation\n",
    "# 7.1  Aggregation on all data\n",
    "flattenDF.agg({\"age\": \"sum\", \"income\": \"mean\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1\n",
    "grouped = flattenDF.select(\"age\", \"income\").groupBy(\"age\")\n",
    "grouped.agg({\"income\" : \"mean\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.2 Some aggregation functions\n",
    "from pyspark.sql.functions import countDistinct, avg\n",
    "grouped.agg(countDistinct(\"age\")).show()\n",
    "grouped.agg(avg(\"age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.3 Dataframe summary\n",
    "flattenDF.describe().show()\n",
    "flattenDF.describe(\"rollno\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.0 An easy way to write table schema:\n",
    "#     Copy the following small file (actots.csv) \n",
    "#     to hdfs:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "hdfs dfs -rm -r -f /user/ashok/data_files/actors\n",
    "hdfs dfs -mkdir -p /user/ashok/data_files/actors\n",
    "hdfs dfs -put /home/ashok/Documents/spark/1.basics/actors.csv  /user/ashok/data_files/actors\n",
    "hdfs dfs -cat /user/ashok/data_files/actors/actors.csv\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# 8.1 File contents are:\n",
    "\n",
    "\"\"\"\n",
    "salman,khan,9899762309,56,01-23-2001\n",
    "kareena,khan,8995634675,45,12-31-2012\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.2 Read the csv file\n",
    "#     For date-pattern symbols (ie m, M, y, HH etc) see:\n",
    "#     https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
    "\n",
    "# 8.2.1\n",
    "from pyspark.sql import *\n",
    "\n",
    "# 8.2.2\n",
    "URL_of_file = \"hdfs://localhost:9000/user/ashok/data_files/actors/\"\n",
    "\n",
    "# 8.2.3\n",
    "df = spark.read.csv(\n",
    "                     path = URL_of_file + \"actors.csv\",\n",
    "                     header = False,           \n",
    "                     sep = \",\",               \n",
    "                     schema = (\"fname string, lname string, phone string, age double, travel date\"),\n",
    "                     dateFormat = \"MM-dd-yyyy\"\n",
    "                   )\n",
    "\n",
    "# 8.2.4\n",
    "df.show()\n",
    "df.dtypes\n",
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# I am done #################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
