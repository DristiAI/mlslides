{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Last amended:30th August, 2021\n",
    "#  Myfolder: /home/ashok/Documents/spark\n",
    "# Ref:\n",
    "# Tutorials (slightly dated):\n",
    "#      https://changhsinlee.com/pyspark-dataframe-basics/\n",
    "#      https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/\n",
    "# Cheat Sheet\n",
    "#      https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf\n",
    "\n",
    "#  Objectives:\n",
    "#           Dataframe operations in spark cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyspark APIs<br>\n",
    "> i)  [DataFrame APIs](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#dataframe-apis)<br>\n",
    ">> df.select(columnName).where(colObject > 30).orderBy(desc(columnName))<br>\n",
    ">> df.select(columnName).where(\"colName > 30\").orderBy(desc(columnName))<br>\n",
    "\n",
    "> ii) [Column APIs](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#column-apis)<br>\n",
    ">> df.select(df.age.isNull())<br>\n",
    ">> df.select(df[\"age\"].isNull())<br>\n",
    ">> df.select(col(\"age\").isNull())<br>\n",
    "\n",
    "> iii)[Data Tyoes](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#data-types)<br>\n",
    "> iv) [Functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#functions)<br>\n",
    ">> df.select(sum(\"age\"))<br>\n",
    ">> df.select(sum(col(booleanColumn).cast(\"int\")))<br>\n",
    ">> <u>but you must import the functions</u>\n",
    "\n",
    "> v)  [Grouping](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#grouping)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. Initial operations:\n",
    "1.0 Start hadoop in a terminal:\n",
    "\n",
    "            ./allstart.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer files to hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Transfer data file 'blackfridayless.csv' to hadoop\n",
    "#     Linux File folder:  /cdata/misc_datasets/black_friday\n",
    "#     In Hadoop first make a folder: /user/ashok/datadir \n",
    "#     and then transfer the file 'blackfridayless.csv' to \n",
    "#     this folder: /user/ashok/datadir\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "cd ~\n",
    "hdfs dfs -rm -f -r  /user/ashok/datadir\n",
    "hdfs dfs -mkdir /user/ashok/datadir\n",
    "hdfs dfs -put /cdata/misc_datasets/black_friday/blackfridayless.csv  /user/ashok/datadir\n",
    "hdfs dfs -ls /user/ashok/datadir\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set jupyter notebook options\n",
    "Start pyspark with jupyter notebook interface. There is no need to create SparkContext and Spark session. pyspark creates them when starting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 Display multiple outputs from a cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1.4 Increase cell width to display wide columnar output\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the csv file from hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Read file 'blackfridayless.csv' from hadoop\n",
    "\n",
    "# What is the URL of my file?\n",
    "URL_of_file= \"hdfs://localhost:9000/user/ashok/datadir/\"\n",
    "\n",
    "# 5.2 Takes time. We use 'spark' session object to read file:\n",
    "blackfriday = spark.read.csv(\n",
    "                             path = URL_of_file + \"blackfridayless.csv\",\n",
    "                             inferSchema = True,      # Default: False\n",
    "                             header = True,           # Default: False\n",
    "                             sep = \",\",               # Default: \",\"\n",
    "                             ignoreLeadingWhiteSpace = True,  # Default: False\n",
    "                             ignoreTrailingWhiteSpace = True  # Default: False\n",
    "    \n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+----+----------+------------+-------------+-------------+-----------+-----------+-----------+--------+\n",
      "| userId|productId|gender| age|occupation|cityCategory|stayCityYears|maritalStatus|productCat1|productCat2|productCat3|purchase|\n",
      "+-------+---------+------+----+----------+------------+-------------+-------------+-----------+-----------+-----------+--------+\n",
      "|1000001|P00069042|     F|0-17|        10|           A|            2|            0|          3|       null|       null|    8370|\n",
      "|1000001|P00248942|     F|0-17|        10|           A|            2|            0|          1|          6|         14|   15200|\n",
      "|1000001|P00087842|     F|0-17|        10|           A|            2|            0|         12|       null|       null|    1422|\n",
      "+-------+---------+------+----+----------+------------+-------------+-------------+-----------+-----------+-----------+--------+\n",
      "only showing top 3 rows\n",
      "\n",
      "['userId', 'productId', 'gender', 'age', 'occupation', 'cityCategory', 'stayCityYears', 'maritalStatus', 'productCat1', 'productCat2', 'productCat3', 'purchase']\n",
      "[('userId', 'int'), ('productId', 'string'), ('gender', 'string'), ('age', 'string'), ('occupation', 'int'), ('cityCategory', 'string'), ('stayCityYears', 'string'), ('maritalStatus', 'int'), ('productCat1', 'int'), ('productCat2', 'int'), ('productCat3', 'int'), ('purchase', 'int')]\n"
     ]
    }
   ],
   "source": [
    "blackfriday.show(3)\n",
    "print(blackfriday.columns)\n",
    "print(blackfriday.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- productId: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- occupation: integer (nullable = true)\n",
      " |-- cityCategory: string (nullable = true)\n",
      " |-- stayCityYears: string (nullable = true)\n",
      " |-- maritalStatus: integer (nullable = true)\n",
      " |-- productCat1: integer (nullable = true)\n",
      " |-- productCat2: integer (nullable = true)\n",
      " |-- productCat3: integer (nullable = true)\n",
      " |-- purchase: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print schema of blackfriday:\n",
    "\n",
    "blackfriday.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+---------+------+------+\n",
      "|summary|            userId|productId|gender|   age|\n",
      "+-------+------------------+---------+------+------+\n",
      "|  count|            100887|   100887|100887|100887|\n",
      "|   mean|1002746.9106723364|     null|  null|  null|\n",
      "| stddev|1678.5336050997462|     null|  null|  null|\n",
      "|    min|           1000001|P00000142|     F|  0-17|\n",
      "|    max|           1006040| P0099942|     M|   55+|\n",
      "+-------+------------------+---------+------+------+\n",
      "\n",
      "+-------+-----------------+------------+------------------+-------------------+\n",
      "|summary|       occupation|cityCategory|     stayCityYears|      maritalStatus|\n",
      "+-------+-----------------+------------+------------------+-------------------+\n",
      "|  count|           100887|      100887|            100887|             100887|\n",
      "|   mean|8.086631577903992|        null|1.4730108801539887|0.40921030459821384|\n",
      "| stddev|6.529747259316683|        null|0.9918812951876441|0.49169058110724656|\n",
      "|    min|                0|           A|                 0|                  0|\n",
      "|    max|               20|           C|                4+|                  1|\n",
      "+-------+-----------------+------------+------------------+-------------------+\n",
      "\n",
      "+-------+-----------------+-----------------+------------------+-----------------+\n",
      "|summary|      productCat1|      productCat2|       productCat3|         purchase|\n",
      "+-------+-----------------+-----------------+------------------+-----------------+\n",
      "|  count|           100887|            69458|             30787|           100887|\n",
      "|   mean|5.300236898708456|9.858864349678942|12.658102445837528|9298.579945880045|\n",
      "| stddev|3.734879811232304|5.084170589056789|4.1247171190814464|4967.311759804714|\n",
      "|    min|                1|                2|                 3|              185|\n",
      "|    max|               18|               18|                18|            23961|\n",
      "+-------+-----------------+-----------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describe the statistics of data:\n",
    "\n",
    "blackfriday.select([\"userId\", \"productId\", \"gender\", \"age\"]).describe().show()\n",
    "blackfriday.select([\"occupation\", \"cityCategory\", \"stayCityYears\", \"maritalStatus\"]).describe().show()\n",
    "blackfriday.select([\"productCat1\", \"productCat2\", \"productCat3\", \"purchase\"]).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5801"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count How many distinct userids are there \n",
    "blackfriday.select([\"userid\"]).distinct().count()\n",
    "# Count how many distinct age-groups exist\n",
    "blackfriday.select([\"age\"]).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many null values occur in each column\n",
    "from pyspark.sql.functions import isnan, isnull,col, sum, max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|userId|\n",
      "+------+\n",
      "|     0|\n",
      "+------+\n",
      "\n",
      "+---------+\n",
      "|productId|\n",
      "+---------+\n",
      "|        0|\n",
      "+---------+\n",
      "\n",
      "+------+\n",
      "|gender|\n",
      "+------+\n",
      "|     0|\n",
      "+------+\n",
      "\n",
      "+---+\n",
      "|age|\n",
      "+---+\n",
      "|  0|\n",
      "+---+\n",
      "\n",
      "+----------+\n",
      "|occupation|\n",
      "+----------+\n",
      "|         0|\n",
      "+----------+\n",
      "\n",
      "+------------+\n",
      "|cityCategory|\n",
      "+------------+\n",
      "|           0|\n",
      "+------------+\n",
      "\n",
      "+-------------+\n",
      "|stayCityYears|\n",
      "+-------------+\n",
      "|            0|\n",
      "+-------------+\n",
      "\n",
      "+-------------+\n",
      "|maritalStatus|\n",
      "+-------------+\n",
      "|            0|\n",
      "+-------------+\n",
      "\n",
      "+-----------+\n",
      "|productCat1|\n",
      "+-----------+\n",
      "|          0|\n",
      "+-----------+\n",
      "\n",
      "+-----------+\n",
      "|productCat2|\n",
      "+-----------+\n",
      "|      31429|\n",
      "+-----------+\n",
      "\n",
      "+-----------+\n",
      "|productCat3|\n",
      "+-----------+\n",
      "|      70100|\n",
      "+-----------+\n",
      "\n",
      "+--------+\n",
      "|purchase|\n",
      "+--------+\n",
      "|       0|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in blackfriday.columns:\n",
    "    blackfriday.select(sum(col(i).isNull().alias(\"nullcol\").cast(\"int\")).alias(i)).show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These columns have null values. Most probably it means that there is no sub-category or sub-categories present. <br>\n",
    "How would you plan to fill them?<br>\n",
    "productCat1 :  0 <br>\n",
    "productCat2 :  31429 <br>\n",
    "productCat3 :  70100 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all integer columns and string columns\n",
    "intCols = [i[0] for i in blackfriday.dtypes  if i[1] == \"int\" ]\n",
    "strCols = [i[0] for i in blackfriday.dtypes  if i[1] == \"string\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['userId', 'occupation', 'maritalStatus', 'productCat1', 'productCat2', 'productCat3', 'purchase']\n",
      "['productId', 'gender', 'age', 'cityCategory', 'stayCityYears']\n"
     ]
    }
   ],
   "source": [
    "print(intCols)\n",
    "print(strCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|max(productCat2)|\n",
      "+----------------+\n",
      "|              18|\n",
      "+----------------+\n",
      "\n",
      "+----------------+\n",
      "|max(productCat3)|\n",
      "+----------------+\n",
      "|              18|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blackfriday.select(max('productCat2')).show()\n",
    "blackfriday.select(max('productCat3')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|max(occupation)|\n",
      "+---------------+\n",
      "|             20|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find minimum and max values of each column\n",
    "blackfriday.select(max(col('occupation'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+-----+----------+------------+-------------+-------------+-----------+-----------+-----------+--------+\n",
      "| userId|productId|gender|  age|occupation|cityCategory|stayCityYears|maritalStatus|productCat1|productCat2|productCat3|purchase|\n",
      "+-------+---------+------+-----+----------+------------+-------------+-------------+-----------+-----------+-----------+--------+\n",
      "|1000001|P00069042|     F| 0-17|        10|           A|            2|            0|          3|        999|       null|    8370|\n",
      "|1000001|P00248942|     F| 0-17|        10|           A|            2|            0|          1|          6|         14|   15200|\n",
      "|1000001|P00087842|     F| 0-17|        10|           A|            2|            0|         12|        999|       null|    1422|\n",
      "|1000001|P00085442|     F| 0-17|        10|           A|            2|            0|         12|         14|       null|    1057|\n",
      "|1000002|P00285442|     M|  55+|        16|           C|           4+|            0|          8|        999|       null|    7969|\n",
      "|1000003|P00193542|     M|26-35|        15|           A|            3|            0|          1|          2|       null|   15227|\n",
      "|1000004|P00184942|     M|46-50|         7|           B|            2|            1|          1|          8|         17|   19215|\n",
      "|1000004|P00346142|     M|46-50|         7|           B|            2|            1|          1|         15|       null|   15854|\n",
      "|1000004| P0097242|     M|46-50|         7|           B|            2|            1|          1|         16|       null|   15686|\n",
      "|1000005|P00274942|     M|26-35|        20|           A|            1|            1|          8|        999|       null|    7871|\n",
      "|1000005|P00251242|     M|26-35|        20|           A|            1|            1|          5|         11|       null|    5254|\n",
      "|1000005|P00014542|     M|26-35|        20|           A|            1|            1|          8|        999|       null|    3957|\n",
      "|1000005|P00031342|     M|26-35|        20|           A|            1|            1|          8|        999|       null|    6073|\n",
      "|1000005|P00145042|     M|26-35|        20|           A|            1|            1|          1|          2|          5|   15665|\n",
      "|1000006|P00231342|     F|51-55|         9|           A|            1|            0|          5|          8|         14|    5378|\n",
      "|1000006|P00190242|     F|51-55|         9|           A|            1|            0|          4|          5|       null|    2079|\n",
      "|1000006| P0096642|     F|51-55|         9|           A|            1|            0|          2|          3|          4|   13055|\n",
      "|1000006|P00058442|     F|51-55|         9|           A|            1|            0|          5|         14|       null|    8851|\n",
      "|1000007|P00036842|     M|36-45|         1|           B|            1|            1|          1|         14|         16|   11788|\n",
      "|1000008|P00249542|     M|26-35|        12|           C|           4+|            1|          1|          5|         15|   19614|\n",
      "+-------+---------+------+-----+----------+------------+-------------+-------------+-----------+-----------+-----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fill null values in productCat2 and productCat3\n",
    "\n",
    "blackfriday.na.fill(999, [\"productCat2\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>productId</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>occupation</th>\n",
       "      <th>cityCategory</th>\n",
       "      <th>stayCityYears</th>\n",
       "      <th>maritalStatus</th>\n",
       "      <th>productCat1</th>\n",
       "      <th>productCat2</th>\n",
       "      <th>productCat3</th>\n",
       "      <th>purchase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000001</td>\n",
       "      <td>P00069042</td>\n",
       "      <td>F</td>\n",
       "      <td>0-17</td>\n",
       "      <td>10</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000001</td>\n",
       "      <td>P00248942</td>\n",
       "      <td>F</td>\n",
       "      <td>0-17</td>\n",
       "      <td>10</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000001</td>\n",
       "      <td>P00087842</td>\n",
       "      <td>F</td>\n",
       "      <td>0-17</td>\n",
       "      <td>10</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000001</td>\n",
       "      <td>P00085442</td>\n",
       "      <td>F</td>\n",
       "      <td>0-17</td>\n",
       "      <td>10</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000002</td>\n",
       "      <td>P00285442</td>\n",
       "      <td>M</td>\n",
       "      <td>55+</td>\n",
       "      <td>16</td>\n",
       "      <td>C</td>\n",
       "      <td>4+</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    userId  productId gender   age  occupation cityCategory stayCityYears  \\\n",
       "0  1000001  P00069042      F  0-17          10            A             2   \n",
       "1  1000001  P00248942      F  0-17          10            A             2   \n",
       "2  1000001  P00087842      F  0-17          10            A             2   \n",
       "3  1000001  P00085442      F  0-17          10            A             2   \n",
       "4  1000002  P00285442      M   55+          16            C            4+   \n",
       "\n",
       "   maritalStatus  productCat1  productCat2  productCat3  purchase  \n",
       "0              0            3          NaN          NaN      8370  \n",
       "1              0            1          6.0         14.0     15200  \n",
       "2              0           12          NaN          NaN      1422  \n",
       "3              0           12         14.0          NaN      1057  \n",
       "4              0            8          NaN          NaN      7969  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform spark dataframe to pandas dataframe:\n",
    "\n",
    "x = blackfriday.toPandas()\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|cityCategory|count|\n",
      "+------------+-----+\n",
      "|           B|42347|\n",
      "|           C|31071|\n",
      "|           A|27469|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show a value count of levels of column 'cityCategory':\n",
    "\n",
    "blackfriday.groupBy(\"cityCategory\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a stratified sampling of data.\n",
    "# Stratified sampling be by column: 'cityCategory'\n",
    "# Take 80% from 'B' and 20% from 'C'\n",
    "\n",
    "sample = blackfriday.sampleBy(\n",
    "                      \"cityCategory\",      # column that defines strata\n",
    "                      fractions = {'B' : 0.8, 'C' : 0.2})   # sampling fraction for each stratum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using verbs\n",
    "select, <i>select(x).where()</i>, <i>select().distinct()</i>, filter, groupby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select syntax\n",
    "> DataFrame.select(\\*cols)<br>\n",
    "> cols: column names (string) or expressions (Column). If one of the column names is ‘*’, that column is expanded to include all columns in the current DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show columns 3rd till 5th\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter syntax\n",
    ">DataFrame.filter(condition)<br>\n",
    ">condition: <i>columnObject > 34</i> or string format: <i>\"age > 34\"</i>\n",
    ">>  df.age > 3 or col(\"age\") > 3<br>\n",
    ">>  \"age > 3\" <br>\n",
    ">>Logical Operators<br>\n",
    ">>> If string: AND OR NOT<br>\n",
    ">>> If columnObject: &, |, ~ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[userId: int, productId: string, gender: string, age: string, occupation: int, cityCategory: string, stayCityYears: string, maritalStatus: int, productCat1: int, productCat2: int, productCat3: int, purchase: int]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[userId: int, productId: string, gender: string, age: string, occupation: int, cityCategory: string, stayCityYears: string, maritalStatus: int, productCat1: int, productCat2: int, productCat3: int, purchase: int]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[userId: int, productId: string, gender: string, age: string, occupation: int, cityCategory: string, stayCityYears: string, maritalStatus: int, productCat1: int, productCat2: int, productCat3: int, purchase: int]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter purchases less than 9000\n",
    "\n",
    "blackfriday.filter(\"purchase < 9000\")\n",
    "blackfriday.where(\"purchase < 9000\")\n",
    "blackfriday.filter(col('purchase') < 9000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35936"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "35936"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "100887"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "35936"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter purchases less than 9000 and maritalStatus is 0\n",
    "\n",
    "blackfriday.filter('purchase < 9000').filter('maritalStatus = 0').count()\n",
    "blackfriday.filter('purchase < 9000').filter('maritalStatus == 0').count()\n",
    "blackfriday.filter('purchase < 9000 AND maritalStatus == 0').count()\n",
    "blackfriday.filter((col('purchase') < 9000) & (col('maritalStatus') == 0)).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84313"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "84313"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter purchases less than 9000 or maritalStatus is 0\n",
    "\n",
    "blackfriday.filter('purchase < 9000 OR maritalStatus == 0').count()\n",
    "blackfriday.filter((col('purchase') < 9000) | (col('maritalStatus') == 0)).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Combining verbs: select, filter and distinct\n",
    "\n",
    "airports_df.select('dst', 'tz'). \\\n",
    "            filter(airports_df.tz == -5). \\\n",
    "            show(3)\n",
    "\n",
    "# 10.1\n",
    "airports_df.select('dst', 'tz'). \\\n",
    "            filter(airports_df.tz == -5). \\\n",
    "            distinct(). \\\n",
    "            show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation with groupby\n",
    "Use: <i>.agg({'colName1' : 'mean', 'colName2' : 'sum'})</i> <br>\n",
    ">With <i>agg()</i> one can use only builtin functions and not any other <i>pyspark.sql.function</i>.<br>\n",
    "Some common functions are: <i>mean, avg, sum, count, first, last,stddev </i>. There is no need to import builtin function in advance.<br>\n",
    "For a complete list of builtin functions see [here](https://sparkbyexamples.com/pyspark/pyspark-aggregate-functions/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute '_get_object_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-b02bf032f9e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mblackfriday\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'maritalStatus'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m'max'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'purchase'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark-3.0.0-bin-hadoop3.2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36magg\u001b[0;34m(self, *exprs)\u001b[0m\n\u001b[1;32m   1544\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1545\u001b[0m         \"\"\"\n\u001b[0;32m-> 1546\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mexprs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1548\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.0-bin-hadoop3.2/python/pyspark/sql/group.py\u001b[0m in \u001b[0;36magg\u001b[0;34m(self, *exprs)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mexprs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exprs should not be empty\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexprs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexprs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexprs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;31m# Columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.0-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1296\u001b[0;31m         \u001b[0margs_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m         \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCALL_COMMAND_NAME\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.0-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_build_args\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_build_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverters\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverters\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1260\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1261\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m             \u001b[0mnew_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.0-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_args\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m   1245\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mconverter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcan_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1247\u001b[0;31m                         \u001b[0mtemp_arg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1248\u001b[0m                         \u001b[0mtemp_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_arg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m                         \u001b[0mnew_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_arg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.0-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_collections.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, object, gateway_client)\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0mjava_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHashMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m             \u001b[0mjava_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjava_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.0-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_collections.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.0-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1296\u001b[0;31m         \u001b[0margs_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m         \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCALL_COMMAND_NAME\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.0-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_build_args\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m         args_command = \"\".join(\n\u001b[0;32m-> 1266\u001b[0;31m             [get_command_part(arg, self.pool) for arg in new_args])\n\u001b[0m\u001b[1;32m   1267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0margs_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.0-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m         args_command = \"\".join(\n\u001b[0;32m-> 1266\u001b[0;31m             [get_command_part(arg, self.pool) for arg in new_args])\n\u001b[0m\u001b[1;32m   1267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0margs_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.0-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_command_part\u001b[0;34m(parameter, python_proxy_pool)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mcommand_part\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\";\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minterface\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mcommand_part\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mparameter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0mcommand_part\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute '_get_object_id'"
     ]
    }
   ],
   "source": [
    "blackfriday.agg({'maritalStatus' : 'max', 'purchase' : max}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. groupby. Can apply sum, min, max, count\n",
    "\n",
    "airports_df.groupby('tz'). \\\n",
    "           count(). \\\n",
    "           show(3)\n",
    "\n",
    "# 12.1\n",
    "airports_df.groupby('tz'). \\\n",
    "            agg({'lat' : 'mean'}). \\\n",
    "            show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Unpacking operator in python (*) : \n",
    "Ref: https://codeyarns.com/2012/04/26/unpack-operator-in-python/ \n",
    "\n",
    "def fox(a,b):  \n",
    "    return (a *b)  \n",
    "\n",
    "m = [3,4]  \n",
    "fox(m)    \n",
    "fox(*m)   \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 One can take the average of columns by passing\n",
    "#       an unpacked list of column names.\n",
    "\n",
    "grObject = airports_df.groupby('tz')\n",
    "\n",
    "avg_cols = ['lat', 'lon']\n",
    "grObject.avg(*avg_cols).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.2 To call multiple aggregation functions at once, pass a dictionary.\n",
    "#         The 'key' of dictionary becomes argument to 'value'.\n",
    "#                             count(*)        avg(lat)      sum(lon)\n",
    "\n",
    "grObject.agg({'*': 'count', 'lat': 'avg', 'lon':'sum'}).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Create new columns in Spark using .withColumn() --mutate\n",
    "#      New column: altInThousands . \n",
    "#      Product of two columns:  'alt' and  'lon' \n",
    "\n",
    "airports_df.withColumn('altInThousands', \n",
    "                       airports_df.alt*airports_df.lon\n",
    "                      ).show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Save the new file with additional column in parquet form\n",
    "\n",
    "xyz = airports_df.withColumn('altInThousands', airports_df.alt*airports_df.lon)\n",
    "xyz.write.parquet(\"hdfs://localhost:9000/user/ashok/data_files/airports_extra.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.1 Delete xyz from spark\n",
    "import gc\n",
    "del xyz\n",
    "gc.collect()    # Delete all cache also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.2 Read the stored parquet file\n",
    "df = spark.read.parquet(\"hdfs://localhost:9000/user/ashok/data_files/airports_extra.parquet\")\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.3 Read 'weather.csv file into spark from hadoop\n",
    "\n",
    "URL_of_file= \"hdfs://localhost:9000/user/ashok/data_files/nycflights/\"\n",
    "weather_df = spark.read.csv(path = URL_of_file + \"weather.csv\",\n",
    "                            inferSchema = True,\n",
    "                            header = True\n",
    "                           )\n",
    "weather_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Joins\n",
    "# Refer: http://www.learnbymarketing.com/1100/pyspark-joins-by-example/\n",
    "# For example, I can join the two titanic dataframes by the column PassengerId\n",
    "\n",
    "# 10.1\n",
    "airports_df.join(weather_df, airports_df.faa==weather_df.origin).show(3)\n",
    "# 10.2\n",
    "airports_df.join(weather_df, airports_df.faa==weather_df.origin, how = 'inner').show(3)\n",
    "# 10.3\n",
    "airports_df.join(weather_df, airports_df.faa==weather_df.origin, how = 'left').show(3)   # Could also use 'left_outer', 'right', 'full'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL queries against DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Many of the operations can be accessed by writing SQL queries in spark.sql().\n",
    "# To make an existing Spark dataframe usable for spark.sql(), one needs to\n",
    "#   register said dataframe as a temporary table.\n",
    "\n",
    "# 11.1 As an example, we can register the two dataframes as temp tables then\n",
    "#      join them through spark.sql().\n",
    "\n",
    "airports_df.createOrReplaceTempView('dfa_temp')\n",
    "weather_df.createOrReplaceTempView('dfw_temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11.2 Simple SQL query. SQLContext is no longer needed. 'spark'\n",
    "#            session object can be used.\n",
    "\n",
    "dfj = spark.sql('select * from dfa_temp' )\n",
    "dfj.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11.3 Now the SQL join\n",
    "\n",
    "dfj = spark.sql('select * from dfa_temp a, dfw_temp b where a.faa = b.origin' )\n",
    "dfj.show(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Drop a columns\n",
    "\n",
    "airports_df.drop('name').show(3)\n",
    "\n",
    "# 12.1  Or drop multiple columns\n",
    "\n",
    "columns_to_drop = ['name', 'lat']\n",
    "xx =airports_df.drop(*columns_to_drop)\n",
    "xx.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### I am done ####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
