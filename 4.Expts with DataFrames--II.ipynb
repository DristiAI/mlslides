{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd /opt/spark-3.0.0-bin-hadoop3.2/python\n",
    "# python setup.py install\n",
    "# Start jupyter notebook\n",
    "# Ref: https://github.com/pinarersoy/PySpark_SparkSQL_MLib/blob/master/PySpark%20and%20SparkSQL.ipynb\n",
    "# Data Source: Kaggle: https://www.kaggle.com/cmenca/new-york-times-hardcover-fiction-best-sellers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.0 Run following command to transfer files\n",
    "#     from localfile system to hadoop\n",
    "hdfs dfs -put /home/ashok/Documents/spark/1.basics/nyt2.json  /user/ashok/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Start SparkSession and create SparkContext\n",
    "#     (no need to execute if you started pyspark)\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Import pyspark related modules\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 Other libraries\n",
    "from datetime import date, timedelta, datetime\n",
    "import pandas as pd\n",
    "import time, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4\n",
    "os.chdir(\"/home/ashok/Documents/spark/1.basics\")\n",
    "os.listdir()\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4\n",
    "help(SparkSession)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.0 Session can be observed at http://localhost:4040\n",
    "#     Tab name is myExpsts\n",
    "#     (no need to execute if you started pyspark)\n",
    "spark = SparkSession.builder \\\n",
    "                    .master(\"local\") \\\n",
    "                    .appName(\"myExpts\") \\\n",
    "                    .getOrCreate()\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Display outputs from multiple commands\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.0 REad json file\n",
    "nyt = spark.read.json(\"/user/ashok/nyt2.json\")\n",
    "# Read TXT FILES\n",
    "# dataframe_txt = sc.read.text('text_data.txt')\n",
    "\n",
    "# Read CSV FILES\n",
    "# dataframe_csv = sc.read.csv('csv_data.csv')\n",
    "\n",
    "# Read PARQUET FILES\n",
    "# dataframe_parquet = sc.read.load('parquet_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.0 To avoid display of overlapping table in jupyter do this:\n",
    "# 4.1. Go to: \n",
    "#           cd  /home/ashok/anaconda3/lib/python3.7/site-packages/notebook/static/style\n",
    "\n",
    "# 4.2. Save file style.min.css as style.min.css.old\n",
    "#    cp style.min.css style.min.css.old\n",
    "\n",
    "# 4.3. Check:\n",
    "#    ls style.min*\n",
    "\n",
    "# 4.4. open file style.min.css:\n",
    "#    leafpad style.min.css \n",
    "#    Search for white-space: pre-wrap;  \n",
    "#    (possibly line numbers: 1568 11443 )\n",
    "#    You will find it at two places. Commentout both the lines,as:\n",
    "#        /* white-space: pre-wrap;   */\n",
    "#    Save the file as style.min.css. \n",
    "# 4.5. Restart jupyter\n",
    "#    See StackOverflow: https://stackoverflow.com/a/60295993\n",
    "# 4.6. Else, use this command to see output vertically:\n",
    "#      nyt.show(n=5, truncate=False, vertical=True)\n",
    "\n",
    "nyt.show(3, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.0 Some Dataframe commands\n",
    "nyt.count()\n",
    "nyt.describe().show()\n",
    "nyt.summary().show()\n",
    "nyt.printSchema()\n",
    "nyt.columns\n",
    "nyt.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Also there is orderBy() function\n",
    "help(nyt.sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Sort on multiple columns:\n",
    "nyt.sort([\"author\", \"publisher\"]).show(5)\n",
    "# OR\n",
    "nyt.sort([\"author\", desc(\"publisher\")]).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 Are there duplicate data?\n",
    "nyt.sort(\"author\").select(\"_id\", \"author\", \"title\").show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.4 Are there duplicate rows ?\n",
    "# Group by \"title\", \"author\"\n",
    "grouped = nyt.groupby([\"title\",\"author\"])     \n",
    "grouped.count().sort( desc(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.5 Drop duplicates\n",
    "nyt = nyt.drop_duplicates()\n",
    "nyt.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyt[nyt.author.isin([\"Dan Brown\", \"Harper Lee\"])].show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.0 Filter commands\n",
    "nyt.where(col('author').isin([\"Dan Brown\", \"Harper Lee\"])).select(\"title\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Use OR not ||\n",
    "nyt.where(\"author == 'Dan Brown'  OR author == 'Harper Lee' \").select(\"author\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 USe of %like%\n",
    "# https://stackoverflow.com/questions/41889974/filter-df-when-values-matches-part-of-a-string-in-pyspark\n",
    "nyt.select(\"author\").filter(\"author like   '%Har%'\").show(3)\n",
    "nyt.filter(\"author like '%Har%'\").select(\"title\").show(3)\n",
    "nyt.filter(\"title startswith \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3 A uniform way to code queries\n",
    "nyt.filter(nyt.author.startswith(\"Har\")).show(3)\n",
    "nyt.filter(nyt.author.endswith(\"en\")).show(3,False)\n",
    "nyt.filter(nyt.author.like(\"%Har%\")).show(3)\n",
    "nyt.filter(nyt.author.isin([\"Harlan Coben\", \"Dan Brown\"])).show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.4 Filteration using 'select'\n",
    "nyt.select(\"author\", nyt.author.startswith(\"Har\")).show(3)\n",
    "nyt.select(\"author\", nyt.author.endswith(\"en\")).show(3,False)\n",
    "nyt.select(\"author\", nyt.author.like(\"%Har%\")).show(3)\n",
    "nyt.select(\"author\", nyt.author.isin([\"Harlan Coben\", \"Dan Brown\"])).show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.0 \n",
    "help(nyt.distinct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1\n",
    "nyt.select(\"author\").distinct().show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.2 Implement pandas value_count()\n",
    "grouped = nyt.groupby(\"author\")\n",
    "grouped.count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.0 Missing values\n",
    "#     DataFrame.na. method has three methods:\n",
    "#     na.drop, na.fill and na.replace\n",
    "#     Methods are .drop() : \n",
    "#                        .drop('any'): Drop a row if 'any' field value is NULL\n",
    "#                        .drop('all'): Drop a row if 'all' field values are NULL\n",
    "#                        .drop(thresh = 5) : Drop a row if at least 5 field values are NULL\n",
    "#                        .drop('any'/'all'/thresh, subset = ['col1', 'col2']): Consider this\n",
    "#                                       column subset for 'any'/'all'/thresh behaviour\n",
    "#               .fill(value): Replace this value everywhere\n",
    "#               .fill(value, subset = ['col1', 'col2']) : Replace value in just these columns\n",
    "#.              .fill({'col1': val1, 'col2' : val2}): Fill null, as here\n",
    "#               .replace(what-to-replace,replace_with,subset)\n",
    "#                   what-to-replace: Value to be replaced. Can be dict {'value': its-replacement}\n",
    "#                                 or a list [23,43] ie replace 23 and 43\n",
    "#                   replace-with: Replace value(s) with what. Can be a list [21,41]\n",
    "#                   subset: ['col1', 'col2']: Consider these columns\n",
    "help(nyt.na)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NaN vs Null in Spark\n",
    "In Spark a null is missing value--or something empty. NaN is, however, not a number such as division by 0. Thus NaN may generally occur when a column has a float or double type. It can occur in any type of column. However, a string such as: \"\" is not a missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1 Handling missing values\n",
    "#     First create a dataframe\n",
    "\n",
    "import numpy as np\n",
    "from pyspark.sql import *\n",
    "\n",
    "row1 =  Row(age = 10,    height = 10.1,           income = 45.9,     name =  \"aAlice\")\n",
    "row2 =  Row(age = 83,    height = None,           income = 45.9,     name =  \"bAlice\")\n",
    "row3 =  Row(age = 30,    height = None,           income = np.nan,   name =  \"cAlice\")\n",
    "row4 =  Row(age = 83,    height = float(83.1),    income = 45.9,     name =  \"dAlice\")\n",
    "row5 =  Row(age = None,  height = None,           income = 45.9,     name =   None   )\n",
    "row6 =  Row(age = None,  height = float(84.2),    income = 45.9,     name =   None   )\n",
    "row7 =  Row(age = 23,    height = None,           income = np.nan,   name =   None   )\n",
    "row8 =  Row(age = 11,    height = float(45.3),    income = np.nan,   name =  \"eAlice\")\n",
    "row8 =  Row(age = 12,    height = None,           income = 45.9,     name =  \"fAlice\")\n",
    "row9 =  Row(age = 33,    height = float(np.nan),  income = 45.9,     name =  None    )\n",
    "row10 = Row(age = 30,    height = float(np.nan),  income = 45.9,     name =  None    )\n",
    "row11 = Row(age = 33,    height = float(33.0),    income = 45.9,     name =  \"\"      )  # Sting \"\"\n",
    "\n",
    "# 8.2 Collect these rows to create a DataFrame\n",
    "df = spark.createDataFrame([row1, row2, row3, row4, row5, row6, row7, row8, row9, row10, row11])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.3 Using fill\n",
    "#     Only two columns are filled int\n",
    "df.na.fill(18).show()\n",
    "\n",
    "# 8.4 Only name column will be filled in\n",
    "#     Note, however, the earlier value\n",
    "#     of \"\" does not get filled in\n",
    "df.na.fill(\"sunder\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.5 Fill only 'age'\n",
    "df.na.fill(18, ['age']).show()\n",
    "\n",
    "# 8.6 Fill 'age' and 'height' with different values\n",
    "df.na.fill({'age': 18, 'height': 87.5}).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.0 Using replace:\n",
    "df.show()\n",
    "\n",
    "# 9.1 Replace 10 by 19 everywhere \n",
    "df.na.replace(10,19).show()\n",
    "\n",
    "# 9.2 Replace 10 by 19 only in 'age'\n",
    "#     column\n",
    "df.na.replace(10,19,'age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.3 Replace 10 by 19 and 83 by 89 everywhere\n",
    "df.na.replace([10,83], [19,89]).show()\n",
    "\n",
    "# 9.4 Replace 10 by 19 and 83 by 89 everywhere\n",
    "df.na.replace([10,83], [19,89], 'age').show()\n",
    "\n",
    "# 9.5 Same as above\n",
    "df.na.replace({10 : 19, 83 : 89 }, 'age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, count, col\n",
    "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### I am done  #######"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
