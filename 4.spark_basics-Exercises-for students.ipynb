{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Last amended:06th September, 2021\n",
    "#  Myfolder: /home/ashok/Documents/spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises for students"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyspark APIs<br>\n",
    "> i)  [DataFrame APIs](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#dataframe-apis)<br>\n",
    ">> df.select(columnName).where(colObject > 30).orderBy(desc(columnName))<br>\n",
    ">> df.select(columnName).where(\"colName > 30\").orderBy(desc(columnName))<br>\n",
    "\n",
    "> ii) [Column APIs](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#column-apis)<br>\n",
    ">> df.select(df.age.isNull())<br>\n",
    ">> df.select(df[\"age\"].isNull())<br>\n",
    ">> df.select(col(\"age\").isNull())<br>\n",
    "\n",
    "> iii)[Data Tyoes](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#data-types)<br>\n",
    "> iv) [Functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#functions)<br>\n",
    ">> df.select(sum(\"age\"))<br>\n",
    ">> df.select(sum(col(booleanColumn).cast(\"int\")))<br>\n",
    ">> <u>but you must import the functions</u>\n",
    "\n",
    "> v)  [Grouping](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#grouping)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. Initial operations:<br>\n",
    "Start hadoop in a terminal:\n",
    "\n",
    "            ./allstart.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer files to hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Transfer data file 'blackfridayless.csv' \n",
    "#     from linux foder:     /cdata/misc_datasets/black_friday/\n",
    "#     to hadoop at:   /user/ashok/datadir/ \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "cd ~\n",
    "hdfs dfs -rm -f -r  /user/ashok/datadir\n",
    "hdfs dfs -mkdir /user/ashok/datadir\n",
    "hdfs dfs -put /cdata/misc_datasets/black_friday/blackfridayless.csv  /user/ashok/datadir\n",
    "hdfs dfs -ls /user/ashok/datadir\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set jupyter notebook options\n",
    "Start pyspark with jupyter notebook interface. There is no need to create SparkContext and Spark session. pyspark creates them when starting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Display multiple outputs from a cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 Increase cell width to display wide columnar output\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the csv file from hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Read file 'blackfridayless.csv' from hadoop\n",
    "# 1.4 Here is a skelton code for you. Fill up question marks (?):\n",
    "\n",
    "blackfriday = spark.read.csv(\n",
    "                             path =  ? ,\n",
    "                             inferSchema = ? ,        # Choices: True. False\n",
    "                             header = True,           # Choices: True, False\n",
    "                             sep = \",\",               \n",
    "                             ignoreLeadingWhiteSpace = ? ,    # Choices: True, False\n",
    "                             ignoreTrailingWhiteSpace = ?     # Choices: True, False\n",
    "    \n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.5   Display few rows of dataframe\n",
    "# 1.5.1 What are the column names\n",
    "# 1.5.2 Show dtypes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.6 Use printSchema() to display schema of blackfriday DataFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.7 Describe the statistics of DataFrame, blackfriday\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.8 Using 'select' clause, display 5-rows of only three columns: userId, gender and age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.9 Count How many distinct userId are there \n",
    "# 1.9.1 Count how many distinct age-groups exist\n",
    "# (use count() and distinct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the following functions\n",
    "from pyspark.sql.functions import isnan, isnull,col, sum, max, min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.0 In productCat2, fill null values with 999 \n",
    "#     Use:  na.fill()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Transform spark dataframe, blackfriday, to pandas dataframe:\n",
    "#     using toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Show the value counts of levels of the column 'cityCategory':\n",
    "#     Use groupBy() and count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 Perform a stratified sampling of data.\n",
    "#     Stratified sampling be by column: 'cityCategory'\n",
    "#     Take 80% from 'B' and 20% from 'C'\n",
    "#     Use method   sampleBy(    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using verbs\n",
    "select, <i>select(x).where()</i>, <i>select().distinct()</i>, filter, groupby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select syntax\n",
    "> DataFrame.select(\\*cols)<br>\n",
    "> cols: column names (string) or expressions (Column). If one of the column names is ‘*’, that column is expanded to include all columns in the current DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.0 Display columns 3rd, 4th and 5th"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter syntax\n",
    ">DataFrame.filter(condition)<br>\n",
    ">condition: <i>columnObject > 34</i> or string format: <i>\"age > 34 AND balance < 60\"</i>\n",
    ">>  df.age > 3 --same as-- col(\"age\") > 3<br>\n",
    ">>  \"age > 3\" <br>\n",
    ">>Logical Operators<br>\n",
    ">>> If string: AND OR NOT<br>\n",
    ">>> If columnObject: &, |, ~ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Filter purchases less than 9000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Filter purchases less than 9000 and maritalStatus is 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Filter purchases less than 9000 or maritalStatus is 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation with groupby\n",
    "Use: <i>.agg({'colName1' : 'mean', 'colName2' : 'sum'})</i> <br>\n",
    ">1. With <i>agg()</i> if you are using dictionary one can then use only builtin functions and not any other <i>pyspark.sql.function</i>.<br>\n",
    "Some common functions are: <i>mean, avg, sum, count, first, last,stddev </i>. There is no need to import builtin function in advance.<br>\n",
    "For a complete list of builtin functions see [here](https://sparkbyexamples.com/pyspark/pyspark-aggregate-functions/).<br>\n",
    "2. You can also use a pyspark.sql.function, as: .agg(sum(df.age), min(col(\"purchase\")) )<br> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.0 Find the max of 'age' column and 'min' of purchase column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Find max and min of purchase\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next on a subset grouped by column\n",
    "Questions:\n",
    ">1. How many airports are there timezone-wise ('tz')\n",
    "2. What is the mean latitude (lat), timezone-wise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.0  Get userID wise average purchases\n",
    "# 5.1  Get productID wise std deviation in purchases\n",
    "# 5.2  Get cityCategory and maritalStatus wise average age and avg purchase\n",
    "\n",
    "from pyspark.sql.functions import avg, expr\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.0 Create a new column in blackfriday which is product of \n",
    "#     purchase and stayCityYears\n",
    "#     Use withColumn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Change the name of age colymn to age_cat\n",
    "\n",
    "blackfriday.withColumnRenamed('age', 'age_cat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.0 From blackfriday drop column 'age'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 From blackfriday, drop two columns, 'age' and 'purchase'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Finish ####################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
