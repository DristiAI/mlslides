{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Last amended: 30th June, 2021\n",
    "#  Myfolder: /home/ashok/Documents/spark\n",
    "# Ref: https://mingchen0919.github.io/learning-apache-spark/categorical-data.html\n",
    "#      https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/\n",
    "\n",
    "# Extracting, transforming and selecting features\n",
    "#  https://spark.apache.org/docs/latest/ml-features#extracting-transforming-and-selecting-features\n",
    "\n",
    "# Objectives:\n",
    "#            1. Dealing with categorical columns\n",
    "#            2. Using StingIndexer, OneHotEncoder\n",
    "#            3. Using VectorAssembler\n",
    "#            4. Using StandardScaler\n",
    "#            5. Using pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A. Create some data\n",
    "\n",
    "# This data frame will be used to demonstrate how to use \n",
    "#                  a) StingIndexer,\n",
    "#                  b) OneHotEncoder, \n",
    "\n",
    "\n",
    "# x1, x2 and y2 are categorical columns type strings.\n",
    "# x3 and y1 are a categorical columns with integers.\n",
    "\n",
    "# x4, x5 are numerical columns \n",
    "\n",
    "\n",
    "# 1.0\n",
    "import pandas as pd\n",
    "\n",
    "# 1.1\n",
    "pdf = pd.DataFrame({\n",
    "                    'x1': ['a','a','b','b', 'b', 'c', 'd','d'],\n",
    "                    'x2': ['apple', 'orange', 'orange','orange', 'peach', 'peach','apple','orange'],\n",
    "                    'x3': [1, 1, 2, 2, 2, 4, 1, 2],\n",
    "                    'x4': [2.4, 2.5, 3.5, 1.4, 2.1,1.5, 3.0, 2.0],\n",
    "                    'x5': [12.4, 22.5, 33.5, 11.4, 42.1,11.5, 23.0, 32.0],\n",
    "                    'y1': [1, 0, 1, 0, 0, 1, 1, 0],\n",
    "                    'y2': ['yes', 'no', 'no', 'yes', 'yes', 'yes', 'no', 'yes']\n",
    "                   })  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2\n",
    "df = spark.createDataFrame(pdf)\n",
    "type(df)           # pyspark.sql.dataframe.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B. About DataFrame\n",
    "# Ref: https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf\n",
    "# 2.0\n",
    "df.show(3)          # Show data\n",
    "df.head(3)\n",
    "df.take(2)         # Show two rows\n",
    "type(df.take(2))   # List of objects: pyspark.sql.types.Row\n",
    "r = df.take(2)\n",
    "r[0]               # First row\n",
    "type(r[0])         # pyspark.sql.types.Row\n",
    "df.describe().show()  # Summary statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. StringIndexer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Syntax](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.StringIndexer.html):\n",
    "<i>class pyspark.ml.feature.StringIndexer(*, inputCol=None, outputCol=None, inputCols=None, outputCols=None, handleInvalid='error', stringOrderType='frequencyDesc')</i>\n",
    "\n",
    "    A label indexer that maps a string column of labels to an ML column of label indices. If the input column is numeric, we cast it to string and index the string values. The indices are in [0, numLabels). By default, this is ordered by label frequencies so the most frequent label gets index 0. The ordering behavior is controlled by setting stringOrderType. Its default value is ‘frequencyDesc’."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StringIndexer maps a string column to an index column that will be treated as a categorical column by spark. <br>\n",
    "The indices start with 0 and are ordered by label frequencies. If it is a numerical column, the column will first<br>\n",
    "be casted to a string column and then indexed by StringIndexer.<br>\n",
    "There are three steps to implement the StringIndexer<br>\n",
    "-      Build the StringIndexer model: specify the input column and output column names.\n",
    "-      Learn the StringIndexer model: fit the model with your data.\n",
    "-      Execute the indexing: call the transform function to execute the indexing process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.0\n",
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1\n",
    "# build indexer. No need to specify dataframe here, just column names\n",
    "#                inputCol and outputCol are not lists:\n",
    "\n",
    "string_indexer = StringIndexer(inputCol='x1',\n",
    "                               outputCol='indexed_x1'\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Learn/fit the model on dataframe:\n",
    "\n",
    "si_model = string_indexer.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Transform the data to a new DataFrame:\n",
    "\n",
    "df_si = si_model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4 Resulting df\n",
    "#     From the result it can be seen that (a, b, c) in column x1 are converted to\n",
    "#     (1.0, 0.0, 2.0). They are ordered by their frequencies in column x1.\n",
    "#     Max freq value is coded as 0.\n",
    "\n",
    "df_si.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. OneHotEncoder\n",
    "\n",
    "<i>class pyspark.ml.feature.OneHotEncoder(*, inputCols=None, outputCols=None, handleInvalid='error', dropLast=True, inputCol=None, outputCol=None)</i>\n",
    "\n",
    "    A one-hot encoder that maps a column of category indices to a column of binary vectors, with at most a single one-value per row that indicates the input category index. For example with 5 categories, an input value of 2.0 would map to an output vector of [0.0, 0.0, 1.0, 0.0]. The last category is not included by default (configurable via dropLast), because it makes the vector entries sum up to one, and hence linearly dependent. So an input value of 4.0 maps to [0.0, 0.0, 0.0, 0.0].\n",
    "\n",
    "    When handleInvalid is configured to ‘keep’, an extra “category” indicating invalid values is added as last category. So when dropLast is true, invalid values are encoded as all-zeros vector.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding maps a categorical feature, represented as a label index,<br>\n",
    "to a binary vector with at most a single one-value indicating the presence of<br>\n",
    "a specific feature value from among the set of all feature values. This encoding<br>\n",
    "allows algorithms which expect continuous features, such as Logistic Regression,<br>\n",
    "to use categorical features. For string type input data, it is common to encode<br>\n",
    "categorical features using StringIndexer first.<br>\n",
    "OneHotEncoderEstimator can transform multiple columns, returning an <br>\n",
    "one-hot-encoded output vector column for each input column. It is common to<br>\n",
    "merge these vectors into a single feature vector using VectorAssembler.<br>\n",
    "\n",
    "\n",
    "Each index is converted to a vector. However, in spark the vector is represented by a<br>\n",
    "sparse vector, becase sparse vector can save a lot of memory.<br>\n",
    "The process of using OneHotEncoder is different to using StingIndexer. <br>\n",
    "There are only two steps.<br>\n",
    "-    i) Build an indexer model\n",
    "-    ii) Execute the indexing by calling transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.0\n",
    "from pyspark.ml.feature import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Build OHEE.    Only specify the input/output columns.:\n",
    "#                    Multiple columns can be specified:\n",
    "\n",
    "onehotencoder = OneHotEncoder(\n",
    "                               inputCols= ['indexed_x1'],\n",
    "                                outputCols=['onehotencoded_x1']\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Transform df_si DataFrame to df_dummy\n",
    "\n",
    "model = onehotencoder.fit(df_si)\n",
    "df_dummy = model.transform(df_si)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 Resulting df\n",
    "# (3,[0],[1.0])  => Vector length: 3, At 0th      position, value is 1\t=  1 0 0 0\n",
    "# (3,[1],[1.0])  => Vector length: 3, At 0th      position, value is 1\t=  0 1 0 0\n",
    "# (3,[2],[1.0])  => Vector length: 3, At second   position, value is 1\t=  0 0 1 0\n",
    "# (3,[],[])\t => Vector length: 3      At 3rd/last position, value is 1\t=  0 0 0 1\n",
    "# There is also:  0 0 0 0 . But for invalid values\n",
    "# When parameter, 'dropLast' is true, invalid values are encoded as all-zeros vector.\n",
    "\n",
    "df_dummy.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple columns handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StringIndexing multiple cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## E. Process all categorical columns with Pipeline\n",
    "#     A Pipeline is a sequence of stages. A stage is an instance which has the property of either fit()\n",
    "#      or transform(). When fitting a Pipeline, the stages get executed in order. The example below shows\n",
    "#       how to use pipeline to process all categorical columns.\n",
    "\n",
    "# 5. List all categorical columns:\n",
    "\n",
    "categorical_cols = ['x1', 'x2', 'x3', 'y1', 'y2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Out column names:\n",
    "\n",
    "stg_out_cols = [\"_\".join([\"indexed\",c]) for c in categorical_cols]\n",
    "stg_out_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 StringIndex all columns at one go:\n",
    "\n",
    "string_indexer = StringIndexer(\n",
    "                               inputCols=categorical_cols,\n",
    "                               outputCols= stg_out_cols\n",
    "                              )\n",
    "\n",
    "# 5.3 Learn/fit the model on dataframe:\n",
    "\n",
    "si_model = string_indexer.fit(df)\n",
    "\n",
    "# 5.4 Transform the data to a new DataFrame:\n",
    "\n",
    "df_si = si_model.transform(df)\n",
    "df_si.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OHE multiple columns\n",
    "This will always follow StringIndexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 OneHotEncode all columns at one go\n",
    "# 6.1.1 First OHE column names\n",
    "\n",
    "ohe_out_cols = ['oheCoded_' + c  for c in categorical_cols]\n",
    "ohe_out_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 Build OHE.    Only specify the input/output columns.:\n",
    "#                    Multiple columns can be specified:\n",
    "\n",
    "onehotencoder = OneHotEncoder(\n",
    "                               inputCols   = stg_out_cols,\n",
    "                                outputCols = ohe_out_cols\n",
    "                             )\n",
    "\n",
    "\n",
    "# 6.3 Transform df_si DataFrame to df_dummy\n",
    "\n",
    "model = onehotencoder.fit(df_si)\n",
    "df_dummy = model.transform(df_si)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.4 Show transformed output:\n",
    "\n",
    "df_dummy.select(df_dummy.columns[:7]).show()\n",
    "df_dummy.select(df_dummy.columns[7:12]).show()\n",
    "df_dummy.select(df_dummy.columns[12:]).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline simple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.2 Create stages of pipeline operations:\n",
    "\n",
    "p=Pipeline(\n",
    "           stages=\n",
    "                   [\n",
    "                    StringIndexer(\n",
    "                                  inputCols= categorical_cols,\n",
    "                                  outputCols= stg_out_cols\n",
    "                                  ),\n",
    "                    \n",
    "                    \n",
    "                    OneHotEncoder(\n",
    "                                   inputCols= stg_out_cols,\n",
    "                                   outputCols= ohe_out_cols\n",
    "                                  )  \n",
    "        \n",
    "                    ]\n",
    "\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.3 Execute pipe\n",
    "dx = p.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.4 Show transformed output:\n",
    "\n",
    "dx.select(dx.columns[:7]).show()\n",
    "dx.select(dx.columns[7:12]).show()\n",
    "dx.select(dx.columns[12:]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.0 Remove categorical columns and StringIndexer cols. Only keep OHE columns:\n",
    "\n",
    "fc = list(\n",
    "           set(df.columns) - set(categorical_cols)\n",
    "         )\n",
    "\n",
    "# 8.1 Append OHE columns\n",
    "fc + ohe_out_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.2 Just these columns and none others\n",
    "dx.select(fc+ohe_out_cols).show()\n",
    "dx = dx.select(fc+ohe_out_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VectorAssembler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VectorAssembler is a transformer that combines a given list of columns into a single vector column. It is useful for combining raw features and features generated by different feature transformers into a single feature vector, in order to train ML models like logistic regression and decision trees. VectorAssembler accepts the following input column types: all numeric types, boolean type, and vector type. In each row, the values of the input columns will be concatenated into a vector in the specified order.\n",
    "\n",
    "Examples\n",
    "\n",
    "Assume that we have a DataFrame with the columns id, hour, mobile, userFeatures, and clicked:\n",
    "\n",
    " id | hour | mobile | userFeatures     | clicked\n",
    "----|------|--------|------------------|---------\n",
    " 0  | 18   | 1.0    | [0.0, 10.0, 0.5] | 1.0\n",
    "\n",
    "userFeatures is a vector column that contains three user features. We want to combine hour, mobile, and userFeatures into a single feature vector called features and use it to predict clicked or not. If we set VectorAssembler’s input columns to hour, mobile, and userFeatures and output column to features, after transformation we should get the following DataFrame:\n",
    "\n",
    " id | hour | mobile | userFeatures     | clicked | features\n",
    "----|------|--------|------------------|---------|-----------------------------\n",
    " 0  | 18   | 1.0    | [0.0, 10.0, 0.5] | 1.0     | [18.0, 1.0, 0.0, 10.0, 0.5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.0\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.1 Using vectorassembler\n",
    "\n",
    "# 9.1 Create object\n",
    "#     Input cols are OHE columns + numerical columns\n",
    "#     It excludes 'target'\n",
    "#     Generally output col name is 'features'\n",
    "\n",
    "vc_demo = VectorAssembler(\n",
    "                          inputCols = fc+ ohe_out_cols,\n",
    "                          outputCol = 'features'\n",
    "                         )\n",
    "\n",
    "# 9.2 vc_demo does not have 'fit' method\n",
    "#     only transform() is available\n",
    "#     So transform the data:\n",
    "\n",
    "df_trans_vc = vc_demo.transform(dx)\n",
    "df_trans_vc.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.3 We add VectorAssembler object to pipe\n",
    "\n",
    "p=Pipeline(\n",
    "           stages=\n",
    "                   [\n",
    "                    StringIndexer(\n",
    "                                  inputCols= categorical_cols,\n",
    "                                  outputCols= stg_out_cols\n",
    "                                  ),\n",
    "                    \n",
    "                    \n",
    "                    OneHotEncoder(\n",
    "                                   inputCols= stg_out_cols,\n",
    "                                   outputCols= ohe_out_cols\n",
    "                                  ),\n",
    "                       \n",
    "                     VectorAssembler(\n",
    "                                      inputCols = fc+ ohe_out_cols,\n",
    "                                      outputCol = 'features'\n",
    "                                    )  \n",
    "        \n",
    "                    ]\n",
    "\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.4 Execute pipe\n",
    "model_pipe = p.fit(df)\n",
    "\n",
    "# 9.4.1\n",
    "df_trans = model_pipe.transform(df)                       \n",
    "\n",
    "# 9.4.2\n",
    "df_trans.show(truncate = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trans.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.4.3\n",
    "df_trans.select(\"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.4.4 Reverse engineering\n",
    "# Convert Vector to DataFrame\n",
    "# Slightly complicated\n",
    "\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "re = df_trans.withColumn(\"myfeatures\", vector_to_array(\"features\"))\n",
    "re.show()\n",
    "re.dtypes\n",
    "\n",
    "# 3.1\n",
    "from pyspark.sql.functions import col\n",
    "dt = re.select(col(\"myfeatures\")[0].alias(\"x1\"),col(\"myfeatures\")[1].alias(\"x2\"))\n",
    "dt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization\n",
    "Refer [here](https://spark.apache.org/docs/latest/ml-features#standardscaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.0\n",
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.1 Instantiate scaler class\n",
    "normalizer = StandardScaler(\n",
    "                              inputCol=\"features\",\n",
    "                              outputCol=\"scaledFeatures\",\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.2 Normalize 'features' column\n",
    "ss_model = normalizer.fit(df_trans)\n",
    "df_ss = ss_model.transform(df_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.3 Our data columns\n",
    "df_ss.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.4 Compare existing and transformed features:\n",
    "\n",
    "df_ss.select('features').show(1,truncate=False)\n",
    "df_ss.select('scaledFeatures').show(1,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11.0 We add StandardScaler to pipe:\n",
    "\n",
    "p=Pipeline(\n",
    "           stages=\n",
    "                   [\n",
    "                    StringIndexer(\n",
    "                                  inputCols= categorical_cols,\n",
    "                                  outputCols= stg_out_cols\n",
    "                                  ),\n",
    "                    \n",
    "                    \n",
    "                    OneHotEncoder(\n",
    "                                   inputCols= stg_out_cols,\n",
    "                                   outputCols= ohe_out_cols\n",
    "                                  ),\n",
    "                       \n",
    "                     VectorAssembler(\n",
    "                                      inputCols = fc+ ohe_out_cols,\n",
    "                                      outputCol = 'features'\n",
    "                                    ),\n",
    "                       \n",
    "                      StandardScaler(\n",
    "                                      inputCol = 'features',\n",
    "                                      outputCol = 'scaledFeatures'\n",
    "                                     )  \n",
    "        \n",
    "                    ]\n",
    "\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11.1 Execute pipe\n",
    "model_pipe = p.fit(df)\n",
    "\n",
    "# 11.2\n",
    "df_trans = model_pipe.transform(df)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11.3 Show transformed output:\n",
    "\n",
    "df_trans.select(df_trans.columns[:7]).show()\n",
    "df_trans.select(df_trans.columns[7:12]).show()\n",
    "df_trans.select(df_trans.columns[12:17]).show()\n",
    "df_trans.select(df_trans.columns[17:]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ I am done ##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12 Reverse engineering\n",
    "#    Convert Vector to DataFrame\n",
    "#    Slightly complicated\n",
    "\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "re = df_trans.withColumn(\"myfeatures\", vector_to_array(\"features\"))\n",
    "re.show()\n",
    "re.dtypes\n",
    "\n",
    "# 3.1\n",
    "from pyspark.sql.functions import col\n",
    "dt = re.select(col(\"myfeatures\")[0].alias(\"x1\"),col(\"myfeatures\")[1].alias(\"x2\"))\n",
    "dt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
